{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdyHSnoj7Iun",
    "outputId": "928d23dd-3bef-4583-c9f2-ebb01087cf93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/training\n"
     ]
    }
   ],
   "source": [
    "# create a seperate folder to store everything\n",
    "!mkdir training\n",
    "%cd training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y55OfxBz8QeP",
    "outputId": "eda7f020-b728-4407-d8f5-61addc8084de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indicTrans'...\n",
      "remote: Enumerating objects: 697, done.\u001b[K\n",
      "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
      "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
      "remote: Total 697 (delta 278), reused 344 (delta 240), pack-reused 297\u001b[K\n",
      "Receiving objects: 100% (697/697), 2.64 MiB | 10.12 MiB/s, done.\n",
      "Resolving deltas: 100% (405/405), done.\n",
      "/content/training/indicTrans\n",
      "Cloning into 'indic_nlp_library'...\n",
      "remote: Enumerating objects: 1399, done.\u001b[K\n",
      "remote: Counting objects: 100% (180/180), done.\u001b[K\n",
      "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
      "remote: Total 1399 (delta 135), reused 144 (delta 117), pack-reused 1219\u001b[K\n",
      "Receiving objects: 100% (1399/1399), 9.57 MiB | 9.27 MiB/s, done.\n",
      "Resolving deltas: 100% (745/745), done.\n",
      "Cloning into 'indic_nlp_resources'...\n",
      "remote: Enumerating objects: 139, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
      "Receiving objects: 100% (139/139), 149.77 MiB | 17.91 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "Updating files: 100% (28/28), done.\n",
      "Cloning into 'subword-nmt'...\n",
      "remote: Enumerating objects: 597, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
      "Receiving objects: 100% (597/597), 252.23 KiB | 2.42 MiB/s, done.\n",
      "Resolving deltas: 100% (357/357), done.\n",
      "/content/training\n"
     ]
    }
   ],
   "source": [
    "# clone the repo for running finetuning\n",
    "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
    "%cd indicTrans\n",
    "# clone requirements repositories\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
    "!git clone https://github.com/rsennrich/subword-nmt.git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziWWl-1a8SMw",
    "outputId": "52998b6c-ec1c-498b-8833-dd3110b81226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  tree\n",
      "0 upgraded, 1 newly installed, 0 to remove and 9 not upgraded.\n",
      "Need to get 47.9 kB of archives.\n",
      "After this operation, 116 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
      "Fetched 47.9 kB in 0s (207 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package tree.\n",
      "(Reading database ... 120880 files and directories currently installed.)\n",
      "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
      "Unpacking tree (2.0.2-1) ...\n",
      "Setting up tree (2.0.2-1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Collecting mock\n",
      "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
      "Collecting indic-nlp-library\n",
      "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Downloading sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
      "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.5)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.9)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
      "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.13.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2023.7.22)\n",
      "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-rtd-theme, sphinx-argparse, indic-nlp-library\n",
      "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.1.0 morfessor-2.0.6 portalocker-2.8.2 sacrebleu-2.3.2 sacremoses-0.1.1 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.3.0 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2.2\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 34933, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "remote: Total 34933 (delta 57), reused 54 (delta 35), pack-reused 34850\u001b[K\n",
      "Receiving objects: 100% (34933/34933), 25.08 MiB | 17.34 MiB/s, done.\n",
      "Resolving deltas: 100% (25347/25347), done.\n",
      "/content/training/fairseq\n",
      "Processing /content/training/fairseq\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.5)\n",
      "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.6.3)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.3.2)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.1)\n",
      "Collecting bitarray (from fairseq==0.12.2)\n",
      "  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.2)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.1.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
      "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
      "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19952958 sha256=5f483e82d9470cd477e2bf0f92d204f8292973ab61dc2ffbc36c0dcbd2a3e68b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wjxfll7w/wheels/a4/17/4c/e4a31b53d5cdb30126df4605a200fe053d490b6418b030c66c\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=b2734db743cffebc4eff49f411c697e69e8116fb9d35fe9229d630884e809dfa\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
      "Successfully built fairseq antlr4-python3-runtime\n",
      "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, hydra-core, fairseq\n",
      "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.3 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->xformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->xformers) (1.3.0)\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.22.post7\n",
      "/content/training\n"
     ]
    }
   ],
   "source": [
    "! sudo apt install tree\n",
    "\n",
    "# Install the necessary libraries\n",
    "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
    "# Install fairseq from source\n",
    "!git clone https://github.com/pytorch/fairseq.git\n",
    "%cd fairseq\n",
    "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
    "!pip install ./\n",
    "! pip install xformers\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCVJ9yWW1-qH",
    "outputId": "d7502bee-a408-4bef-e784-e1798d6d0e49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING:xformers:Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: triton is not available\n"
     ]
    }
   ],
   "source": [
    "# add fairseq folder to python path\n",
    "import os\n",
    "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
    "# sanity check to see if fairseq is installed\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmfGYkd58UiO",
    "outputId": "8cf6830e-4616-4c2a-c3a0-798165f73501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-18 09:15:14--  https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/samanatar-en-indic-v0.2.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.125.207, 142.250.136.207, 142.250.148.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.125.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3376507188 (3.1G) [application/zip]\n",
      "Saving to: ‘samanatar-en-indic-v0.2.zip’\n",
      "\n",
      "samanatar-en-indic- 100%[===================>]   3.14G   114MB/s    in 31s     \n",
      "\n",
      "2023-11-18 09:15:45 (103 MB/s) - ‘samanatar-en-indic-v0.2.zip’ saved [3376507188/3376507188]\n",
      "\n",
      "Archive:  samanatar-en-indic-v0.2.zip\n",
      "   creating: final_data/\n",
      "   creating: final_data/en-kn/\n",
      "  inflating: final_data/en-kn/train.kn  \n",
      "  inflating: final_data/en-kn/train.en  \n",
      "   creating: final_data/en-bn/\n",
      "  inflating: final_data/en-bn/train.en  \n",
      "  inflating: final_data/en-bn/train.bn  \n",
      "   creating: final_data/en-as/\n",
      "  inflating: final_data/en-as/train.as  \n",
      "  inflating: final_data/en-as/train.en  \n",
      "   creating: final_data/en-ta/\n",
      "  inflating: final_data/en-ta/train.ta  \n",
      "  inflating: final_data/en-ta/train.en  \n",
      "   creating: final_data/en-or/\n",
      "  inflating: final_data/en-or/train.or  \n",
      "  inflating: final_data/en-or/train.en  \n",
      "   creating: final_data/en-mr/\n",
      "  inflating: final_data/en-mr/train.mr  \n",
      "  inflating: final_data/en-mr/train.en  \n",
      "   creating: final_data/en-te/\n",
      "  inflating: final_data/en-te/train.te  \n",
      "  inflating: final_data/en-te/train.en  \n",
      "   creating: final_data/en-hi/\n",
      "  inflating: final_data/en-hi/train.hi  \n",
      "  inflating: final_data/en-hi/train.en  \n",
      "   creating: final_data/en-pa/\n",
      "  inflating: final_data/en-pa/train.pa  \n",
      "  inflating: final_data/en-pa/train.en  \n",
      "   creating: final_data/en-gu/\n",
      "  inflating: final_data/en-gu/train.gu  \n",
      "  inflating: final_data/en-gu/train.en  \n",
      "   creating: final_data/en-ml/\n",
      "  inflating: final_data/en-ml/train.ml  \n",
      "  inflating: final_data/en-ml/train.en  \n"
     ]
    }
   ],
   "source": [
    "## for the latest samanantar dataset v0.3 -> please use this link: https://storage.googleapis.com/samanantar-public/V0.3/source_wise_splits.zip\n",
    "# This v0.3 dataset has source wise splits to indicate where the data has been collected from\n",
    "# For preprocessing simplicity we will use v0.2( which just uses raw text files without source information) in this tutorial\n",
    "#\n",
    "#\n",
    "#  lets now download the indictrans data v0.2 dataset\n",
    "! wget https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/samanatar-en-indic-v0.2.zip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training data is organized as en-X folders where each folder contains two text files containing parallel data for en-X lang pair.\n",
    "\n",
    "# final_data\n",
    "# ├── en-as\n",
    "# │   ├── train.as\n",
    "# │   └── train.en\n",
    "# ├── en-bn\n",
    "# │   ├── train.bn\n",
    "# │   └── train.en\n",
    "# ├── en-gu\n",
    "# │   ├── train.en\n",
    "# │   └── train.gu\n",
    "# ├── en-hi\n",
    "# │   ├── train.en\n",
    "# │   └── train.hi\n",
    "# ├── en-kn\n",
    "# │   ├── train.en\n",
    "# │   └── train.kn\n",
    "# ├── en-ml\n",
    "# │   ├── train.en\n",
    "# │   └── train.ml\n",
    "# ├── en-mr\n",
    "# │   ├── train.en\n",
    "# │   └── train.mr\n",
    "# ├── en-or\n",
    "# │   ├── train.en\n",
    "# │   └── train.or\n",
    "# ├── en-pa\n",
    "# │   ├── train.en\n",
    "# │   └── train.pa\n",
    "# ├── en-ta\n",
    "# │   ├── train.en\n",
    "# │   └── train.ta\n",
    "# └── en-te\n",
    "#     ├── train.en\n",
    "#     └── train.te\n",
    "\n",
    "\n",
    "! unzip samanatar-en-indic-v0.2.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnq2OqSwC5vd",
    "outputId": "06e09a11-ed20-4a5f-d2f6-9291f751bbd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  benchmarks.zip\n",
      "   creating: benchmarks/\n",
      "   creating: benchmarks/pmi/\n",
      "   creating: benchmarks/pmi/en-as/\n",
      "  inflating: benchmarks/pmi/en-as/dev.as  \n",
      "  inflating: benchmarks/pmi/en-as/dev.en  \n",
      "  inflating: benchmarks/pmi/en-as/test.as  \n",
      "  inflating: benchmarks/pmi/en-as/test.en  \n",
      "   creating: benchmarks/wat2021-devtest/\n",
      "  inflating: benchmarks/wat2021-devtest/dev.gu  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.en  \n",
      "  inflating: benchmarks/wat2021-devtest/test.bn  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.bn  \n",
      "  inflating: benchmarks/wat2021-devtest/test.hi  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.kn  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.ta  \n",
      "  inflating: benchmarks/wat2021-devtest/test.pa  \n",
      "  inflating: benchmarks/wat2021-devtest/test.en  \n",
      "  inflating: benchmarks/wat2021-devtest/test.mr  \n",
      "  inflating: benchmarks/wat2021-devtest/test.kn  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.ml  \n",
      "  inflating: benchmarks/wat2021-devtest/test.ta  \n",
      "  inflating: benchmarks/wat2021-devtest/test.gu  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.or  \n",
      "  inflating: benchmarks/wat2021-devtest/test.or  \n",
      "  inflating: benchmarks/wat2021-devtest/test.te  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.mr  \n",
      "  inflating: benchmarks/wat2021-devtest/test.ml  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.pa  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.te  \n",
      "  inflating: benchmarks/wat2021-devtest/dev.hi  \n",
      "   creating: benchmarks/wat2020-devtest/\n",
      "   creating: benchmarks/wat2020-devtest/en-bn/\n",
      "  inflating: benchmarks/wat2020-devtest/en-bn/dev.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-bn/test.bn  \n",
      "  inflating: benchmarks/wat2020-devtest/en-bn/dev.bn  \n",
      "  inflating: benchmarks/wat2020-devtest/en-bn/test.en  \n",
      "   creating: benchmarks/wat2020-devtest/en-ta/\n",
      "  inflating: benchmarks/wat2020-devtest/en-ta/dev.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-ta/dev.ta  \n",
      "  inflating: benchmarks/wat2020-devtest/en-ta/test.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-ta/test.ta  \n",
      "   creating: benchmarks/wat2020-devtest/en-mr/\n",
      "  inflating: benchmarks/wat2020-devtest/en-mr/dev.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-mr/test.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-mr/test.mr  \n",
      "  inflating: benchmarks/wat2020-devtest/en-mr/dev.mr  \n",
      "   creating: benchmarks/wat2020-devtest/en-te/\n",
      "  inflating: benchmarks/wat2020-devtest/en-te/dev.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-te/test.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-te/test.te  \n",
      "  inflating: benchmarks/wat2020-devtest/en-te/dev.te  \n",
      "   creating: benchmarks/wat2020-devtest/en-hi/\n",
      "  inflating: benchmarks/wat2020-devtest/en-hi/dev.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-hi/test.hi  \n",
      "  inflating: benchmarks/wat2020-devtest/en-hi/test.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-hi/dev.hi  \n",
      "   creating: benchmarks/wat2020-devtest/en-gu/\n",
      "  inflating: benchmarks/wat2020-devtest/en-gu/dev.gu  \n",
      "  inflating: benchmarks/wat2020-devtest/en-gu/dev.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-gu/test.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-gu/test.gu  \n",
      "   creating: benchmarks/wat2020-devtest/en-ml/\n",
      "  inflating: benchmarks/wat2020-devtest/en-ml/dev.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-ml/test.en  \n",
      "  inflating: benchmarks/wat2020-devtest/en-ml/dev.ml  \n",
      "  inflating: benchmarks/wat2020-devtest/en-ml/test.ml  \n",
      "   creating: benchmarks/ufal-ta/\n",
      "   creating: benchmarks/ufal-ta/en-ta/\n",
      "  inflating: benchmarks/ufal-ta/en-ta/dev.en  \n",
      "  inflating: benchmarks/ufal-ta/en-ta/dev.ta  \n",
      "  inflating: benchmarks/ufal-ta/en-ta/test.en  \n",
      "  inflating: benchmarks/ufal-ta/en-ta/test.ta  \n",
      "   creating: benchmarks/wmt-news/\n",
      "   creating: benchmarks/wmt-news/en-ta/\n",
      "  inflating: benchmarks/wmt-news/en-ta/dev.en  \n",
      "  inflating: benchmarks/wmt-news/en-ta/dev.ta  \n",
      "  inflating: benchmarks/wmt-news/en-ta/test.en  \n",
      "  inflating: benchmarks/wmt-news/en-ta/test.ta  \n",
      "   creating: benchmarks/wmt-news/en-hi/\n",
      "  inflating: benchmarks/wmt-news/en-hi/dev.en  \n",
      "  inflating: benchmarks/wmt-news/en-hi/test.hi  \n",
      "  inflating: benchmarks/wmt-news/en-hi/test.en  \n",
      "  inflating: benchmarks/wmt-news/en-hi/dev.hi  \n",
      "   creating: benchmarks/wmt-news/en-gu/\n",
      "  inflating: benchmarks/wmt-news/en-gu/test.en  \n",
      "  inflating: benchmarks/wmt-news/en-gu/test.gu  \n"
     ]
    }
   ],
   "source": [
    "# benchmarks folder consists of all the benchmarks we report in the paper - pmi, ufal-ta, wat2020, wat2021, wmt-news\n",
    "\n",
    "! unzip benchmarks.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhvYXUc1FaVn",
    "outputId": "84f5e786-a637-49de-a577-d2b03804ba9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment ../indic-en-exp on indic to en\n",
      "Applying normalization and script conversion for train\n",
      "100% 138353/138353 [00:17<00:00, 7981.73it/s]\n",
      "100% 138353/138353 [00:33<00:00, 4161.55it/s]\n",
      "Number of sentences in train: 138353\n",
      "Applying normalization and script conversion for dev\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-as/dev.as'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-as/dev.en'\n",
      "Number of sentences in dev: \n",
      "Applying normalization and script conversion for test\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-as/test.as'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-as/test.en'\n",
      "Number of sentences in test: \n",
      "Applying normalization and script conversion for train\n",
      "100% 8435355/8435355 [16:34<00:00, 8484.31it/s]\n",
      "100% 8435355/8435355 [34:03<00:00, 4128.89it/s]\n",
      "Number of sentences in train: 8435355\n",
      "Applying normalization and script conversion for dev\n",
      "100% 2000/2000 [00:00<00:00, 4498.60it/s]\n",
      "100% 2000/2000 [00:00<00:00, 4661.15it/s]\n",
      "Number of sentences in dev: 2000\n",
      "Applying normalization and script conversion for test\n",
      "100% 3522/3522 [00:00<00:00, 7483.12it/s]\n",
      "100% 3522/3522 [00:00<00:00, 5201.85it/s]\n",
      "Number of sentences in test: 3522\n",
      "Applying normalization and script conversion for train\n",
      "100% 8466307/8466307 [21:36<00:00, 6531.99it/s]\n",
      "100% 8466307/8466307 [40:09<00:00, 3513.98it/s]\n",
      "Number of sentences in train: 8466307\n",
      "Applying normalization and script conversion for dev\n",
      "100% 2000/2000 [00:00<00:00, 3987.10it/s]\n",
      "100% 2000/2000 [00:00<00:00, 2858.90it/s]\n",
      "Number of sentences in dev: 2000\n",
      "Applying normalization and script conversion for test\n",
      "100% 3169/3169 [00:00<00:00, 6541.84it/s]\n",
      "100% 3169/3169 [00:00<00:00, 3968.67it/s]\n",
      "Number of sentences in test: 3169\n",
      "Applying normalization and script conversion for train\n",
      "100% 3019563/3019563 [04:53<00:00, 10274.96it/s]\n",
      "100% 3019563/3019563 [10:40<00:00, 4711.15it/s]\n",
      "Number of sentences in train: 3019563\n",
      "Applying normalization and script conversion for dev\n",
      "100% 2000/2000 [00:00<00:00, 4539.45it/s]\n",
      "100% 2000/2000 [00:00<00:00, 3139.93it/s]\n",
      "Number of sentences in dev: 2000\n",
      "Applying normalization and script conversion for test\n",
      "100% 4463/4463 [00:01<00:00, 3996.72it/s]\n",
      "100% 4463/4463 [00:00<00:00, 5095.46it/s]\n",
      "Number of sentences in test: 4463\n",
      "Applying normalization and script conversion for train\n",
      "100% 4014931/4014931 [06:40<00:00, 10031.80it/s]\n",
      "100% 4014931/4014931 [14:06<00:00, 4745.44it/s]\n",
      "Number of sentences in train: 4014931\n",
      "Applying normalization and script conversion for dev\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-kn/dev.kn'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-kn/dev.en'\n",
      "Number of sentences in dev: \n",
      "Applying normalization and script conversion for test\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-kn/test.kn'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-kn/test.en'\n",
      "Number of sentences in test: \n",
      "Applying normalization and script conversion for train\n",
      "100% 5780479/5780479 [10:45<00:00, 8959.00it/s] \n",
      "100% 5780479/5780479 [21:31<00:00, 4474.36it/s]\n",
      "Number of sentences in train: 5780479\n",
      "Applying normalization and script conversion for dev\n",
      "100% 2000/2000 [00:00<00:00, 4372.61it/s]\n",
      "100% 2000/2000 [00:00<00:00, 3370.52it/s]\n",
      "Number of sentences in dev: 2000\n",
      "Applying normalization and script conversion for test\n",
      "100% 2886/2886 [00:00<00:00, 3439.31it/s]\n",
      "100% 2886/2886 [00:00<00:00, 3023.94it/s]\n",
      "Number of sentences in test: 2886\n",
      "Applying normalization and script conversion for train\n",
      "100% 3288874/3288874 [06:13<00:00, 8802.44it/s]\n",
      "100% 3288874/3288874 [12:18<00:00, 4452.16it/s]\n",
      "Number of sentences in train: 3288874\n",
      "Applying normalization and script conversion for dev\n",
      "100% 2000/2000 [00:00<00:00, 6102.59it/s]\n",
      "100% 2000/2000 [00:00<00:00, 4582.88it/s]\n",
      "Number of sentences in dev: 2000\n",
      "Applying normalization and script conversion for test\n",
      "100% 3760/3760 [00:00<00:00, 5276.86it/s]\n",
      "100% 3760/3760 [00:00<00:00, 4231.73it/s]\n",
      "Number of sentences in test: 3760\n",
      "Applying normalization and script conversion for train\n",
      "100% 990439/990439 [01:57<00:00, 8395.48it/s] \n",
      "100% 990439/990439 [03:47<00:00, 4345.24it/s]\n",
      "Number of sentences in train: 990439\n",
      "Applying normalization and script conversion for dev\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-or/dev.or'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-or/dev.en'\n",
      "Number of sentences in dev: \n",
      "Applying normalization and script conversion for test\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-or/test.or'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-or/test.en'\n",
      "Number of sentences in test: \n",
      "Applying normalization and script conversion for train\n",
      "100% 2400659/2400659 [04:57<00:00, 8073.88it/s] \n",
      "100% 2400659/2400659 [09:43<00:00, 4116.22it/s]\n",
      "Number of sentences in train: 2400659\n",
      "Applying normalization and script conversion for dev\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-pa/dev.pa'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-pa/dev.en'\n",
      "Number of sentences in dev: \n",
      "Applying normalization and script conversion for test\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-pa/test.pa'\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"/content/training/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-pa/test.en'\n",
      "Number of sentences in test: \n",
      "Applying normalization and script conversion for train\n",
      "100% 5095763/5095763 [10:39<00:00, 7968.85it/s]\n",
      "100% 5095763/5095763 [20:28<00:00, 4148.90it/s]\n",
      "Number of sentences in train: 5095763\n",
      "Applying normalization and script conversion for dev\n",
      "100% 2000/2000 [00:00<00:00, 3571.49it/s]\n",
      "100% 2000/2000 [00:00<00:00, 2272.29it/s]\n",
      "Number of sentences in dev: 2000\n",
      "Applying normalization and script conversion for test\n",
      "100% 3637/3637 [00:01<00:00, 3334.00it/s]\n",
      "100% 3637/3637 [00:00<00:00, 4337.79it/s]\n",
      "Number of sentences in test: 3637\n",
      "Applying normalization and script conversion for train\n",
      "100% 4775516/4775516 [08:01<00:00, 9915.90it/s]\n",
      "100% 4775516/4775516 [16:46<00:00, 4743.21it/s]\n",
      "Number of sentences in train: 4775516\n",
      "Applying normalization and script conversion for dev\n",
      "100% 2000/2000 [00:00<00:00, 4857.33it/s]\n",
      "100% 2000/2000 [00:00<00:00, 4660.79it/s]\n",
      "Number of sentences in dev: 2000\n",
      "Applying normalization and script conversion for test\n",
      "100% 3049/3049 [00:00<00:00, 7772.96it/s]\n",
      "100% 3049/3049 [00:00<00:00, 5765.53it/s]\n",
      "Number of sentences in test: 3049\n",
      "\n",
      "../indic-en-exp/data/train.SRC\n",
      "../indic-en-exp/data/train.TGT\n",
      "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
      "../indic-en-exp/norm/as-en/train.as\n",
      "../indic-en-exp/norm/as-en/train.en\n",
      "  9% 1/11 [00:00<00:02,  4.60it/s]src: bn, tgt:en\n",
      "../indic-en-exp/norm/bn-en/train.bn\n",
      "../indic-en-exp/norm/bn-en/train.en\n",
      " 18% 2/11 [00:22<02:00, 13.44s/it]src: gu, tgt:en\n",
      "../indic-en-exp/norm/gu-en/train.gu\n",
      "../indic-en-exp/norm/gu-en/train.en\n",
      " 27% 3/11 [00:28<01:19,  9.91s/it]src: hi, tgt:en\n",
      "../indic-en-exp/norm/hi-en/train.hi\n",
      "../indic-en-exp/norm/hi-en/train.en\n",
      " 36% 4/11 [00:57<02:02, 17.44s/it]src: kn, tgt:en\n",
      "../indic-en-exp/norm/kn-en/train.kn\n",
      "../indic-en-exp/norm/kn-en/train.en\n",
      " 45% 5/11 [01:05<01:24, 14.17s/it]src: ml, tgt:en\n",
      "../indic-en-exp/norm/ml-en/train.ml\n",
      "../indic-en-exp/norm/ml-en/train.en\n",
      " 55% 6/11 [01:18<01:08, 13.60s/it]src: mr, tgt:en\n",
      "../indic-en-exp/norm/mr-en/train.mr\n",
      "../indic-en-exp/norm/mr-en/train.en\n",
      " 64% 7/11 [01:27<00:48, 12.00s/it]src: or, tgt:en\n",
      "../indic-en-exp/norm/or-en/train.or\n",
      "../indic-en-exp/norm/or-en/train.en\n",
      " 73% 8/11 [01:28<00:26,  8.74s/it]src: pa, tgt:en\n",
      "../indic-en-exp/norm/pa-en/train.pa\n",
      "../indic-en-exp/norm/pa-en/train.en\n",
      " 82% 9/11 [01:35<00:16,  8.19s/it]src: ta, tgt:en\n",
      "../indic-en-exp/norm/ta-en/train.ta\n",
      "../indic-en-exp/norm/ta-en/train.en\n",
      " 91% 10/11 [01:48<00:09,  9.51s/it]src: te, tgt:en\n",
      "../indic-en-exp/norm/te-en/train.te\n",
      "../indic-en-exp/norm/te-en/train.en\n",
      "100% 11/11 [01:58<00:00, 10.80s/it]\n",
      "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
      "../indic-en-exp/norm/as-en/train.as\n",
      "  9% 1/11 [00:00<00:01,  5.10it/s]src: bn, tgt:en\n",
      "../indic-en-exp/norm/bn-en/train.bn\n",
      " 18% 2/11 [00:10<00:56,  6.32s/it]src: gu, tgt:en\n",
      "../indic-en-exp/norm/gu-en/train.gu\n",
      " 27% 3/11 [00:13<00:37,  4.64s/it]src: hi, tgt:en\n",
      "../indic-en-exp/norm/hi-en/train.hi\n",
      " 36% 4/11 [00:29<01:03,  9.09s/it]src: kn, tgt:en\n",
      "../indic-en-exp/norm/kn-en/train.kn\n",
      " 45% 5/11 [00:32<00:42,  7.08s/it]src: ml, tgt:en\n",
      "../indic-en-exp/norm/ml-en/train.ml\n",
      " 55% 6/11 [00:39<00:34,  6.87s/it]src: mr, tgt:en\n",
      "../indic-en-exp/norm/mr-en/train.mr\n",
      " 64% 7/11 [00:44<00:24,  6.24s/it]src: or, tgt:en\n",
      "../indic-en-exp/norm/or-en/train.or\n",
      " 73% 8/11 [00:45<00:14,  4.75s/it]src: pa, tgt:en\n",
      "../indic-en-exp/norm/pa-en/train.pa\n",
      " 82% 9/11 [00:49<00:08,  4.45s/it]src: ta, tgt:en\n",
      "../indic-en-exp/norm/ta-en/train.ta\n",
      " 91% 10/11 [00:55<00:04,  4.99s/it]src: te, tgt:en\n",
      "../indic-en-exp/norm/te-en/train.te\n",
      "100% 11/11 [01:00<00:00,  5.46s/it]\n",
      "\n",
      "../indic-en-exp/data/dev.SRC\n",
      "../indic-en-exp/data/dev.TGT\n",
      "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
      "src: bn, tgt:en\n",
      "../indic-en-exp/norm/bn-en/dev.bn\n",
      "../indic-en-exp/norm/bn-en/dev.en\n",
      "src: gu, tgt:en\n",
      "../indic-en-exp/norm/gu-en/dev.gu\n",
      "../indic-en-exp/norm/gu-en/dev.en\n",
      "src: hi, tgt:en\n",
      "../indic-en-exp/norm/hi-en/dev.hi\n",
      "../indic-en-exp/norm/hi-en/dev.en\n",
      "src: kn, tgt:en\n",
      "src: ml, tgt:en\n",
      "../indic-en-exp/norm/ml-en/dev.ml\n",
      "../indic-en-exp/norm/ml-en/dev.en\n",
      "src: mr, tgt:en\n",
      "../indic-en-exp/norm/mr-en/dev.mr\n",
      "../indic-en-exp/norm/mr-en/dev.en\n",
      "src: or, tgt:en\n",
      "src: pa, tgt:en\n",
      "src: ta, tgt:en\n",
      "../indic-en-exp/norm/ta-en/dev.ta\n",
      "../indic-en-exp/norm/ta-en/dev.en\n",
      "src: te, tgt:en\n",
      "../indic-en-exp/norm/te-en/dev.te\n",
      "../indic-en-exp/norm/te-en/dev.en\n",
      "100% 11/11 [00:00<00:00, 134.85it/s]\n",
      "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
      "src: bn, tgt:en\n",
      "../indic-en-exp/norm/bn-en/dev.bn\n",
      "src: gu, tgt:en\n",
      "../indic-en-exp/norm/gu-en/dev.gu\n",
      "src: hi, tgt:en\n",
      "../indic-en-exp/norm/hi-en/dev.hi\n",
      "src: kn, tgt:en\n",
      "src: ml, tgt:en\n",
      "../indic-en-exp/norm/ml-en/dev.ml\n",
      "src: mr, tgt:en\n",
      "../indic-en-exp/norm/mr-en/dev.mr\n",
      "src: or, tgt:en\n",
      "src: pa, tgt:en\n",
      "src: ta, tgt:en\n",
      "../indic-en-exp/norm/ta-en/dev.ta\n",
      "src: te, tgt:en\n",
      "../indic-en-exp/norm/te-en/dev.te\n",
      "100% 11/11 [00:00<00:00, 681.19it/s]\n",
      "\n",
      "../indic-en-exp/data/test.SRC\n",
      "../indic-en-exp/data/test.TGT\n",
      "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
      "src: bn, tgt:en\n",
      "../indic-en-exp/norm/bn-en/test.bn\n",
      "../indic-en-exp/norm/bn-en/test.en\n",
      "src: gu, tgt:en\n",
      "../indic-en-exp/norm/gu-en/test.gu\n",
      "../indic-en-exp/norm/gu-en/test.en\n",
      "src: hi, tgt:en\n",
      "../indic-en-exp/norm/hi-en/test.hi\n",
      "../indic-en-exp/norm/hi-en/test.en\n",
      "src: kn, tgt:en\n",
      "src: ml, tgt:en\n",
      "../indic-en-exp/norm/ml-en/test.ml\n",
      "../indic-en-exp/norm/ml-en/test.en\n",
      "src: mr, tgt:en\n",
      "../indic-en-exp/norm/mr-en/test.mr\n",
      "../indic-en-exp/norm/mr-en/test.en\n",
      "src: or, tgt:en\n",
      "src: pa, tgt:en\n",
      "src: ta, tgt:en\n",
      "../indic-en-exp/norm/ta-en/test.ta\n",
      "../indic-en-exp/norm/ta-en/test.en\n",
      "src: te, tgt:en\n",
      "../indic-en-exp/norm/te-en/test.te\n",
      "../indic-en-exp/norm/te-en/test.en\n",
      "100% 11/11 [00:00<00:00, 130.03it/s]\n",
      "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
      "src: bn, tgt:en\n",
      "../indic-en-exp/norm/bn-en/test.bn\n",
      "src: gu, tgt:en\n",
      "../indic-en-exp/norm/gu-en/test.gu\n",
      "src: hi, tgt:en\n",
      "../indic-en-exp/norm/hi-en/test.hi\n",
      "src: kn, tgt:en\n",
      "src: ml, tgt:en\n",
      "../indic-en-exp/norm/ml-en/test.ml\n",
      "src: mr, tgt:en\n",
      "../indic-en-exp/norm/mr-en/test.mr\n",
      "src: or, tgt:en\n",
      "src: pa, tgt:en\n",
      "src: ta, tgt:en\n",
      "../indic-en-exp/norm/ta-en/test.ta\n",
      "src: te, tgt:en\n",
      "../indic-en-exp/norm/te-en/test.te\n",
      "100% 11/11 [00:00<00:00, 409.70it/s]\n",
      "Sat Nov 18 02:06:23 PM UTC 2023\n",
      "Input file: ../indic-en-exp/data/train\n",
      "learning source BPE\n",
      "learn_single_bpe.sh: line 22: 77478 Killed                  python $SUBWORD_NMT_DIR/subword_nmt/learn_bpe.py --input $train_file.SRC -s $num_operations -o $expdir/vocab/bpe_codes.32k.SRC --num-workers -1\n",
      "learning target BPE\n",
      "100% 32000/32000 [10:03<00:00, 53.00it/s]\n",
      "computing SRC vocab\n",
      "Error: invalid line 1 in BPE codes file: \n",
      "The line should exist of exactly two subword units, separated by whitespace\n",
      "computing TGT vocab\n",
      "y\n",
      "ok\n",
      "Sat Nov 18 04:10:03 PM UTC 2023\n",
      "train\n",
      "Apply to SRC corpus\n",
      "Error: invalid line 1 in BPE codes file: \n",
      "The line should exist of exactly two subword units, separated by whitespace\n",
      "Apply to TGT corpus\n",
      "/content/training/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
      "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
      "dev\n",
      "Apply to SRC corpus\n",
      "Error: invalid line 1 in BPE codes file: \n",
      "The line should exist of exactly two subword units, separated by whitespace\n",
      "Apply to TGT corpus\n",
      "/content/training/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
      "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
      "test\n",
      "Apply to SRC corpus\n",
      "Error: invalid line 1 in BPE codes file: \n",
      "The line should exist of exactly two subword units, separated by whitespace\n",
      "Apply to TGT corpus\n",
      "/content/training/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
      "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
      "Adding language tags\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2023-11-18 16:40:22.366042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING:xformers:Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: triton is not available\n"
     ]
    }
   ],
   "source": [
    "# prepare_data_joint_training.sh takes experiment dir, src_lang, tgt_lang as input\n",
    "# This does preprocessing, building vocab, binarization for joint training\n",
    "\n",
    "# The learning  and applying vocabulary will take a while if the dataset is huge. To make it faster, run it on a multicore system\n",
    "\n",
    "! bash prepare_data_joint_training.sh '../indic-en-exp' 'indic' 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MR_2GQoa84Jn"
   },
   "outputs": [],
   "source": [
    "# create an experiment dir to store train data, devtest data.\n",
    "# This folder will also store vocabulary files (created with subword_nmt for bpe), fairseq bin files (for training), model checkpoints.\n",
    "\n",
    "# for this example we will be training indic to en translation model. We will name our exp_dir as indic-en-exp\n",
    "! mkdir indic-en-exp\n",
    "# copying all the train folders to exp_dir\n",
    "! cp -r final_data/* indic-en-exp\n",
    "\n",
    "! mkdir -p indic-en-exp/devtest\n",
    "\n",
    "# copying all benchmarks to devtest folder in exp_dir\n",
    "! cp -r benchmarks/* indic-en-exp/devtest\n",
    "\n",
    "# folder to store combined devtest data (based on the domains you want to test, you can combine multiple benchmarks dev datasets, remove duplicates)\n",
    "! mkdir -p indic-en-exp/devtest/all\n",
    "\n",
    "# in this tutorial, for simplicity, we will just use wat2020 devtest for dev and test set\n",
    "! cp -r indic-en-exp/devtest/wat2020-devtest/* indic-en-exp/devtest/all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lorcT8wkFPtQ",
    "outputId": "a3ea6587-bae9-4b16-a42f-477eb9e0c87b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/training/indicTrans\n"
     ]
    }
   ],
   "source": [
    "%cd indicTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p1i3fRQzF2-x"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "# pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
    "\n",
    "\n",
    "# some notable args:\n",
    "# --max-updates         -> maximum update steps the model will be trained for\n",
    "# --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
    "# --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
    "# --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
    "# --max_tokens          -> this is max tokens per batch. You should limit to lower values if you get oom errors.\n",
    "# --update-freq         -> gradient accumulation steps\n",
    "\n",
    "\n",
    "!( fairseq-train ../indic-en-exp/final_bin \\\n",
    "--max-source-positions=210 \\\n",
    "--max-target-positions=210 \\\n",
    "--max-update=<max_updates> \\\n",
    "--save-interval=1 \\\n",
    "--arch=transformer_4x \\\n",
    "--criterion=label_smoothed_cross_entropy \\\n",
    "--source-lang=SRC \\\n",
    "--lr-scheduler=inverse_sqrt \\\n",
    "--target-lang=TGT \\\n",
    "--label-smoothing=0.1 \\\n",
    "--optimizer adam \\\n",
    "--adam-betas \"(0.9, 0.98)\" \\\n",
    "--clip-norm 1.0 \\\n",
    "--warmup-init-lr 1e-07 \\\n",
    "--lr 0.0005 \\\n",
    "--warmup-updates 4000 \\\n",
    "--dropout 0.2 \\\n",
    "--save-dir ../indic-en-exp/model \\\n",
    "--keep-last-epochs 5 \\\n",
    "--patience 5 \\\n",
    "--skip-invalid-size-inputs-valid-test \\\n",
    "--fp16 \\\n",
    "--user-dir model_configs \\\n",
    "--wandb-project <wandb_project_name> \\\n",
    "--update-freq=<grad_accumulation_steps> \\\n",
    "--distributed-world-size <num_gpus> \\\n",
    "--max-tokens <max_tokens_in_a_batch> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaQG6g7LwRMC",
    "outputId": "1dcab331-7474-4e2b-c9f9-16b2969fb54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English to French:\n",
      "BLEU Score: 0.30\n",
      "METEOR Score: 0.50\n",
      "\n",
      "German to English:\n",
      "BLEU Score: 0.25\n",
      "METEOR Score: 0.45\n",
      "\n",
      "Chinese to English:\n",
      "BLEU Score: 0.15\n",
      "METEOR Score: 0.38\n",
      "\n",
      "Spanish to Portuguese:\n",
      "BLEU Score: 0.28\n",
      "METEOR Score: 0.48\n",
      "\n",
      "Japanese to Korean:\n",
      "BLEU Score: 0.22\n",
      "METEOR Score: 0.42\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def calculate_scores(reference_translations, predicted_translations, language_pair):\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = corpus_bleu(reference_translations, predicted_translations)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_score_val = meteor_score(reference_translations, predicted_translations)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"{language_pair}:\\nBLEU Score: {bleu_score:.2f}\\nMETEOR Score: {meteor_score_val:.2f}\\n\")\n",
    "\n",
    "\n",
    "# Calculate and print scores for each language pair\n",
    "calculate_scores(english_to_french_references, english_to_french_predictions, \"English to French\")\n",
    "calculate_scores(german_to_english_references, german_to_english_predictions, \"German to English\")\n",
    "calculate_scores(chinese_to_english_references, chinese_to_english_predictions, \"Chinese to English\")\n",
    "calculate_scores(spanish_to_portuguese_references, spanish_to_portuguese_predictions, \"Spanish to Portuguese\")\n",
    "calculate_scores(japanese_to_korean_references, japanese_to_korean_predictions, \"Japanese to Korean\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
